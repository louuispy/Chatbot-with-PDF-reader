# Assistant AI
## Envie seus PDFs e receba insights incr√≠veis!

### Selecionar idioma:
- [English]()
- [Portugu√™s]()

---

### O que √© o AssistantAI?
O AssistantAI consiste em um projeto desenvolvido por mim, **100% em Python**, para aprofundar meus estudos em IA e colocar em pr√°tica meus conhecimentos de Python, Streamlit e Langchain.

A proposta do chatbot √© responder perguntas do usu√°rio, fornecendo insights interessantes e precisos, com base em arquivos PDF enviados pelo pr√≥prio usu√°rio para a IA.

### Tecnologias utilizadas:
- Streamlit
- Langchain
- Groq API (Llama3)
- HuggingFace Transformers
- PyPDFLoader

### Como utilizar?
  1. Fa√ßa um clone desse reposit√≥rio git para a sua m√°quina
  2. Crie um ambiente virtual com Python e ative-o
      
     ```
     python -m venv venv
     venv\Scripts\activate 
     ```
     
     No caso de Linux ou MacOS, ative o venv assim:
     
     ```
     source venv/bin/activate
     ```
   3. Instale as bibliotecas presentes no `requirements.txt` utilizando o `pip install`
      
      ```
      pip install -r requirements.txt
      ```
   4. Configure as vari√°veis de ambiente, adicionando um `.env` na raiz do projeto, junto √† sua API do Groq Llama3.
   5. Execute o app com
      ```
      streamlit run .\app.py
      ```
---

### Como o projeto foi desenvolvido?
O desenvolvimento do chatbot foi dividido em 3 etapas:
  1. Desenvolver a interface da aplica√ß√£o.
  2. Desenvolver um sistema de RAG com langchain, para receber os PDFs, criar chunks a partir deles.
  3. Desenvolver o sistema de chatbot usando uma API do modelo de linguagem Llama3, por meio do Groq.

---

### 1. Desenvolvendo a interface
Para desenvolver a interface, utilizei `Streamlit`.
Optei por criar uma interface relativamente simples, apenas com o *promp*, para o usu√°rio mandar as mensagens, e uma *sidebar*, para o envio e processamento de PDFs.
Ap√≥s o envio de uma mensagem, ser√£o exibidos na tela as mensagens do user, e logo em seguida da IA.

![image](https://github.com/user-attachments/assets/d7a61bca-70ea-466a-9b52-4f2d7fefa243)

Para que a interface exibisse uma mensagem ap√≥s a outra, em formato de chat, utilizei o `st.session_state`, do `Streamlit`.
```python
if 'messages' not in st.session_state:
        st.session_state.messages = []
for message in st.session_state.messages:
        st.chat_message(message['role']).markdown(message['content'])
```
Basicamente, se n√£o houvesse nenhuma mensagem no `session_state`, ele cria uma lista vazia e, com base o usu√°rio vai enviando prompts e a IA respondendo, as mensagens s√£o armazenadas nesta mesma lista vazia criada.

A partir do momento que uma nova mensagem √© salva na lista `st.session_state.messages`, ela √© exibida na tela, respeitando o *role*, ou seja, quem enviou a mensagem, e o *context*, que consiste no prompt do usu√°rio e na resposta gerada pela IA.

### 2. Desenvolvendo o RAG
Para o RAG, criei um sistema onde √© criado um arquivo tempor√°rio do PDF enviado pelo usu√°rio pela interface em `Streamlit`, para assim, ter acesso ao arquivo e a partir disso, fazer a leitura do PDF e armazenar o conte√∫do lido em uma vari√°vel `loaders`, que √© uma lista vazia.
Com isso, parti para a cria√ß√£o de uma function para criar um `vectorstore`, utilizando o `VectorstoreIndexCreator` e o `HuggingFaceEmbeddings`, ambos importados do `Langchain`.
A partir desse fun√ß√£o, s√£o criados os chunks do arquivo PDF passado pelo usu√°rio. Esses chunks ser√£o utilizados para "alimentar" o modelo de linguagem do Llama3.

### 3. Cria√ß√£o da estrutura de chatbot
Com a etapa de RAG conclu√≠da, partimos ent√£o para a cria√ß√£o da function de conversa√ß√£o com o chatbot, onde criaremos uma chain que permitir√° uma conversa do usu√°rio com a LLM alimentada pelos chunks do PDF.
Para isso, utilizei o `ChatGroq`, ou apenas `Groq`, importando tamb√©m do `Langchain`, e o modelo de linguagem *Llama3*.
O sistema em si da conversa√ß√£o foi simples, feito a partir de um template com *system* - onde passo informa√ß√µes de como o modelo deve responder, e baseado em que - e *user*, onde temos a *question* do usu√°rio.
Uma funcionalidade interessante implementada na function de conversa√ß√£o foi o `ConversationBufferMemory`, que permite que o modelo tenha uma mem√≥ria de conversa. Ou seja, ela consegue responder v√°rias mensagens no prompt com mem√≥ria das mensagens anteriores.

Ap√≥s essas etapas, foram feitas as devidas implementa√ß√µes na interface da aplica√ß√£o, onde s√£o inseridos o processamento dos PDFs e a conversa√ß√£o com a LLM.

---

### üë®üèª‚Äçüíª Autor
Lu√≠s Henrique
UX/UI Designer e Dev apaixonado por IA, Vis√£o Computacional e Experi√™ncia do Usu√°rio.

[Conecte-se comigo no LinkedIn](https://www.linkedin.com/in/luishenrique-ia/)
