# Assistant AI
## Envie seus PDFs e receba insights incr√≠veis!

### Selecionar idioma:
- [English](https://github.com/louuispy/Chatbot-with-PDF-reader/blob/main/README-en.md)
- [Portugu√™s](https://github.com/louuispy/Chatbot-with-PDF-reader/blob/main/README.md)

---

### O que √© o AssistantAI?
O AssistantAI consiste em um projeto desenvolvido por mim, **100% em Python**, para aprofundar meus estudos em IA e colocar em pr√°tica meus conhecimentos de Python, Streamlit e Langchain.

A proposta do chatbot √© responder perguntas do usu√°rio, fornecendo insights interessantes e precisos, com base em arquivos PDF enviados pelo pr√≥prio usu√°rio para a IA.

### Tecnologias utilizadas:
- Streamlit
- Langchain
- Groq API (Llama3)
- HuggingFace Transformers
- PyPDFLoader

### Como utilizar?
  1. Fa√ßa um clone desse reposit√≥rio git para a sua m√°quina
  2. Crie um ambiente virtual com Python e ative-o
      
     ```
     python -m venv venv
     venv\Scripts\activate 
     ```
     
     No caso de Linux ou MacOS, ative o venv assim:
     
     ```
     source venv/bin/activate
     ```
   3. Instale as bibliotecas presentes no `requirements.txt` utilizando o `pip install`
      
      ```
      pip install -r requirements.txt
      ```
   4. Configure as vari√°veis de ambiente, adicionando um `.env` na raiz do projeto, junto √† sua API do Groq Llama3.
   5. Execute o app com
      ```
      streamlit run .\app.py
      ```
---
### Case de uso do AssistantAI
Um case que voc√™ pode fazer, ap√≥s realizar as etapas anteriores, consiste em:
1. Acesse a *sidebar* e envie um PDF de curr√≠culo, ou at√© mesmo apostila de algum concurso que esteja fazendo;
2. Ap√≥s o envio, clique em Processar PDFs, para que o(s) arquivo(s) seja(m) processado(s) pela IA;
3. Se tudo ocorreu certo, voc√™ receber√° uma mensagem indicando que os arquivos foram processados com sucesso.
4. Ap√≥s isso, fa√ßa qualquer pergunta relacionada ao(s) PDF(s) para a IA, e ela responder√°, trazendo os insights necess√°rios e de forma precisa.

> [!NOTE] 
> Alguns arquivos PDF podem n√£o ser processados corretamente.
> Esse ponto ser√° corrigido em futuras atualiza√ß√µes do projeto.

---

### Como o projeto foi desenvolvido?
O desenvolvimento do chatbot foi dividido em 3 etapas:
  1. Desenvolver a interface da aplica√ß√£o.
  2. Desenvolver um sistema de RAG com langchain, para receber os PDFs, criar chunks a partir deles.
  3. Desenvolver o sistema de chatbot usando uma API do modelo de linguagem Llama3, por meio do Groq.

---

### 1. Desenvolvendo a interface
Para desenvolver a interface, utilizei `Streamlit`.
Optei por criar uma interface relativamente simples, apenas com o *promp*, para o usu√°rio mandar as mensagens, e uma *sidebar*, para o envio e processamento de PDFs.
Ap√≥s o envio de uma mensagem, ser√£o exibidos na tela as mensagens do user, e logo em seguida da IA.

![image](https://github.com/user-attachments/assets/d7a61bca-70ea-466a-9b52-4f2d7fefa243)

Para que a interface exibisse uma mensagem ap√≥s a outra, em formato de chat, utilizei o `st.session_state`, do `Streamlit`.
```python
if 'messages' not in st.session_state:
        st.session_state.messages = []
for message in st.session_state.messages:
        st.chat_message(message['role']).markdown(message['content'])
```
Basicamente, se n√£o houver nenhuma mensagem no `session_state`, ele cria uma lista vazia e, com base o usu√°rio vai enviando prompts e a IA respondendo, as mensagens s√£o armazenadas nesta mesma lista vazia criada.

A partir do momento que uma nova mensagem √© salva na lista `st.session_state.messages`, ela √© exibida na tela, respeitando o *role*, ou seja, quem enviou a mensagem, e o *context*, que consiste no prompt do usu√°rio e na resposta gerada pela IA.

---

### 2. Desenvolvendo o RAG
Para o RAG, criei um sistema onde √© criado um arquivo tempor√°rio do PDF enviado pelo usu√°rio pela interface em `Streamlit`, para assim, ter acesso ao arquivo e a partir disso, fazer a leitura do PDF e armazenar o conte√∫do lido em uma vari√°vel `loaders`, que √© uma lista vazia.
```python
def create_vectorstore(uploaded_files):
    loaders = []
    for pdf in uploaded_files:
        with tempfile.NamedTemporaryFile(delete=False, suffix=".pdf") as tmp:
            tmp.write(pdf.read())
            tmp_path = tmp.name
        loaders.append(PyPDFLoader(tmp_path))
```

Com isso, ainda nessa fun√ß√£o `create_vectorstore` parti para a cria√ß√£o de uma function para criar um `vectorstore`, utilizando o `VectorstoreIndexCreator` e o `HuggingFaceEmbeddings`, ambos importados do `Langchain`.
A partir desse fun√ß√£o, s√£o criados os chunks do arquivo PDF passado pelo usu√°rio. Esses chunks ser√£o utilizados para "alimentar" o modelo de linguagem do Llama3.
```python
index=VectorstoreIndexCreator(
        embedding=HuggingFaceEmbeddings(model_name='all-MiniLM-L12-v2'),
        text_splitter=RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)
    ).from_loaders(loaders)

    return index.vectorstore
```
---

### 3. Cria√ß√£o da estrutura de chatbot
Com a etapa de RAG conclu√≠da, partimos ent√£o para a cria√ß√£o da function de conversa√ß√£o com o chatbot, onde criaremos uma chain que permitir√° uma conversa do usu√°rio com a LLM alimentada pelos chunks do PDF.
Para isso, utilizei o `ChatGroq`, ou apenas `Groq`, importado tamb√©m do `Langchain`, e o modelo de linguagem *Llama3*.

O sistema em si da conversa√ß√£o foi simples, feito a partir de um template com *system* - onde passo informa√ß√µes de como o modelo deve responder, e baseado em que - e *user*, onde temos a *question* do usu√°rio.
```python
system_template = '''Voc√™ √© um assistente de intelig√™ncia artificial simp√°tico e profissional chamado Assistant. Voc√™ responde em Portugu√™s-BR.
Voc√™ sempre responde de forma clara, objetiva e precisa as d√∫vidas dos usu√°rios. Voc√™ responde com base no contexto: {context}'''

prompt_template = ChatPromptTemplate.from_messages([
        ("system", system_template),
        ("user", "{question}")
        ])
```

Uma funcionalidade interessante implementada na function de conversa√ß√£o foi o `ConversationBufferMemory`, que permite que o modelo tenha uma mem√≥ria de conversa. Ou seja, ela consegue responder v√°rias mensagens no prompt com mem√≥ria das mensagens anteriores.
```python
memory = ConversationBufferMemory(memory_key='chat_history', return_messages=True)
```
Ap√≥s essas etapas, foram feitas as devidas implementa√ß√µes na interface da aplica√ß√£o, onde s√£o inseridos o processamento dos PDFs e a conversa√ß√£o com a LLM.

---

### üë®üèª‚Äçüíª Autor
Lu√≠s Henrique

UX/UI Designer e Dev apaixonado por IA, Vis√£o Computacional e Experi√™ncia do Usu√°rio.

[Conecte-se comigo no LinkedIn](https://www.linkedin.com/in/luishenrique-ia/)
